# Code Review Fixes - WebSocket Reconnection

## Overview

This document details the fixes applied in response to a thorough code review of the WebSocket reconnection implementation. The review identified 3 critical issues that would have caused problems in production.

## Findings and Fixes

### Finding 1 (High): Subscription replay responses still forwarded to clients

**Problem**:
- When reconnecting, we replay subscriptions to the new upstream
- We correctly added replayed subscriptions to `pending_subscribes`
- **BUT** `handle_upstream_message` still forwarded the subscription response to the client
- This caused duplicate JSON-RPC responses for old request IDs after reconnect
- Clients receive the same subscription response twice, breaking their JSON-RPC state

**Root Cause**:
```rust
// In handle_upstream_message (BEFORE fix)
if let Some((params, _)) = pending.remove(&id_str) {
    // Track the subscription
    tracker.lock().await.track_subscribe(params, id.clone(), sub_id);
    VixyMetrics::inc_ws_subscriptions();
    debug!(sub_id, "Tracked new subscription");
    // âŒ BUG: Still forwards response to client below!
}

// Later in the same function...
client_sender.lock().await.send(Message::Text(text_to_send.into())).await
```

**Fix**:
```rust
// In handle_upstream_message (AFTER fix)
if let Some((params, _)) = pending.remove(&id_str) {
    // âœ… Issue #2 Fix (Finding 1): This is a replayed subscription response
    // Track it internally but DO NOT forward to client (they already got it)
    tracker.lock().await.track_subscribe(params, id.clone(), sub_id);
    VixyMetrics::inc_ws_subscriptions();
    debug!(sub_id, "Tracked replayed subscription (not forwarding response)");

    // Return early - don't forward this response to client
    return Ok(());
}
```

**Impact**: Prevents duplicate subscription responses that would break client JSON-RPC state.

**File**: `src/proxy/ws.rs:640-650`

---

### Finding 2 (Medium): Message queueing doesn't work during reconnection

**Problem**:
- Reconnection was awaited inline in the select loop
- While `reconnect_upstream().await` is running, the select loop is blocked
- The loop cannot process `client_msg_rx.recv()`
- Messages from clients pile up in the channel buffer, NOT in our queue
- When reconnection finishes, messages were lost (not queued)

**Root Cause**:
```rust
// BEFORE fix - reconnection blocks the select loop
Some(reconnect_info) = reconnect_rx.recv() => {
    is_reconnecting.store(true, Ordering::SeqCst);

    // âŒ BUG: This await blocks the entire loop!
    // While waiting, we can't process client messages
    match reconnect_upstream(...).await {
        Ok(...) => { /* ... */ }
        Err(e) => { /* ... */ }
    }
}
```

**Fix**:
```rust
// AFTER fix - spawn reconnection as background task
Some(reconnect_info) = reconnect_rx.recv() => {
    is_reconnecting.store(true, Ordering::SeqCst);

    // âœ… Spawn as background task - main loop continues!
    let (reconnect_tx, rx) = oneshot::channel();
    reconnect_result_rx = Some(rx);

    tokio::spawn(async move {
        let result = reconnect_upstream(...).await;
        let _ = reconnect_tx.send(result);
    });

    info!("Reconnection task spawned, main loop continues processing messages");
}

// Separate select branch handles reconnection completion
Ok(result) = async { reconnect_result_rx.as_mut().unwrap().await },
    if reconnect_result_rx.is_some() => {
    reconnect_result_rx = None;
    // Process result (replay queue or clear queue)
}
```

**Impact**:
- Main loop continues processing client messages during reconnection
- Messages are properly queued using the `is_reconnecting` flag
- Zero message loss during reconnection window

**Files**:
- `src/proxy/ws.rs:14` (added `oneshot` import)
- `src/proxy/ws.rs:338-339` (added reconnect_result_rx option)
- `src/proxy/ws.rs:374-409` (spawn reconnection task)
- `src/proxy/ws.rs:411-469` (handle reconnection completion)

---

### Finding 3 (Medium): Queue not cleared on reconnection failure

**Problem**:
- If reconnection fails, we cleared `is_reconnecting` flag
- **BUT** we didn't clear `message_queue`
- Stale messages would remain in the queue
- Next successful reconnection would replay these stale messages
- Could lead to messages being sent to wrong upstream or in wrong order

**Root Cause**:
```rust
// BEFORE fix
Err(e) => {
    VixyMetrics::inc_ws_reconnection_attempt("failed");
    is_reconnecting.store(false, Ordering::SeqCst);
    // âŒ BUG: Queue not cleared! Stale messages remain.
    error!(error = %e, "Failed to reconnect WebSocket upstream");
}
```

**Fix**:
```rust
// AFTER fix
Err(e) => {
    VixyMetrics::inc_ws_reconnection_attempt("failed");

    // âœ… Fix Finding 3: Clear queue AND flag on reconnection failure
    is_reconnecting.store(false, Ordering::SeqCst);
    let mut queue = message_queue.lock().await;
    let dropped_count = queue.len();
    queue.clear();
    if dropped_count > 0 {
        warn!(count = dropped_count,
              "Dropped queued messages due to reconnection failure");
    }

    error!(error = %e, "Failed to reconnect WebSocket upstream");
}
```

**Impact**: Prevents stale messages from being replayed after failed reconnection.

**File**: `src/proxy/ws.rs:453-467`

---

## Summary of Changes

### Files Modified
1. `src/proxy/ws.rs` - All three fixes applied

### Key Changes
1. **Early return** for replayed subscription responses (don't forward to client)
2. **Spawned reconnection** as background task (main loop continues)
3. **Added oneshot channel** to communicate reconnection result
4. **Clear message queue** on reconnection failure

### Testing
- All 92 unit tests pass âœ…
- Clippy clean (no warnings) âœ…
- Integration tests: Not yet run (require Kurtosis infrastructure)

## Architectural Improvements

The fixes implement better concurrent programming patterns:

1. **Non-blocking reconnection**: Background task allows main loop to continue
2. **Message queueing actually works**: Main loop processes messages during reconnect
3. **Proper cleanup**: Queue cleared on failure prevents stale message replay
4. **Response suppression**: Replayed subscription responses consumed internally

## Risk Assessment

**Before fixes**:
- ðŸ”´ High: Duplicate subscription responses break clients (Finding 1)
- ðŸŸ¡ Medium: Messages lost during reconnection (Finding 2)
- ðŸŸ¡ Medium: Stale messages replayed after failed reconnect (Finding 3)

**After fixes**:
- âœ… All three issues resolved
- âœ… Message queueing now functional
- âœ… Proper concurrent task management

## Reviewer Credit

These critical issues were identified by an independent code review. The reviewer's note:

> "If the intent is to keep queueing during reconnect, consider moving reconnect into a spawned task so the main loop keeps polling the client receiver."

This was exactly the right solution and has been implemented.

---

**Date**: January 23, 2026
**Reviewer**: Independent code review
**Implementer**: Claude Sonnet 4.5
